# Differentiable Neural Computer (DNC) for Meta-Learning & RL

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


**Core Idea:** An RL agent augmented with external, addressable, differentiable memory, designed to solve complex sequential tasks and potentially exhibit faster adaptation (meta-learning properties). This implementation aims to replicate core concepts from DeepMind's DNC paper within an RL framework.

**Target Claim:** Solve certain procedural tasks (e.g., mazes) with significantly fewer episodes (~40% goal) compared to standard LSTM baselines by leveraging learned memory access patterns.

## Key Features & Innovations

ðŸ§  **External Differentiable Memory:**
   - Controller (LSTM) interacts with an `N x M` memory matrix.
   - Attention-based **Read Heads** using sparse content addressing (Top-K).
   - Attention-based **Write Head** using content *and* allocation-based addressing (writing to unused slots).
   - Implements **Usage Tracking** and **Temporal Link Matrix** updates as per the DNC paper.

ðŸ“‰ **Reinforcement Learning Integration:**
   - Trained using **Actor-Critic (A2C)** with Generalized Advantage Estimation (GAE).
   - Compatible with Gymnasium environments.
   - Includes `ProceduralMazeEnv` and `RepeatCopyEnv` examples.

ðŸ“Š **Benchmarking Framework:**
   - Includes an LSTM baseline for comparison.
   - Config-driven training (`scripts/train.py`) via YAML files.
   - Logging via TensorBoard (`runs/` directory).
   - Tools for memory visualization (`scripts/visualize.py`).

## Project Structure

```plaintext
dnc-meta-learning/
â”œâ”€â”€ environments/      # Gymnasium environment implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ maze_env.py    # Procedural maze navigation
â”‚   â””â”€â”€ algo_env.py    # Algorithmic tasks (e.g., RepeatCopy)
â”œâ”€â”€ models/            # PyTorch model definitions
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dnc.py         # Core DNC architecture
â”‚   â”œâ”€â”€ lstm_baseline.py # Baseline comparison model
â”‚   â”œâ”€â”€ memory_heads.py# Attention-based read/write heads
â”‚   â””â”€â”€ utils.py       # Helper functions (cosine similarity, etc.)
â”œâ”€â”€ tests/             # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_memory.py # Tests for memory utility functions
â”œâ”€â”€ scripts/           # Executable scripts
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train.py       # Main training loop (A2C)
â”‚   â””â”€â”€ visualize.py   # Generate memory access visualizations
â”œâ”€â”€ configs/           # Hyperparameter configuration files
â”‚   â”œâ”€â”€ dnc.yaml
â”‚   â””â”€â”€ lstm.yaml
â”œâ”€â”€ docs/              # Documentation and results
â”‚   â”œâ”€â”€ MEMORY.md      # Details on the DNC memory mathematics
â”‚   â””â”€â”€ RESULTS.md     # Placeholder for benchmark results and graphs
â”œâ”€â”€ deploy/            # Deployment examples (Experimental)
â”‚   â”œâ”€â”€ export_onnx.py # Script to export model to ONNX (TODO)
â”‚   â””â”€â”€ ros_integration_example.py # Conceptual ROS integration (TODO)
â”œâ”€â”€ saved_models/      # Default location for saved model checkpoints
â”œâ”€â”€ runs/              # Default location for TensorBoard logs
â””â”€â”€ README.md          # This file```

## Quick Start

**1. Clone & Setup Environment:**

```bash
git clone https://github.com/your-username/dnc-meta-learning.git # TODO: Update URL
cd dnc-meta-learning

# Create conda environment (recommended)
conda create -n dnc python=3.9
conda activate dnc

# Install dependencies
# Adjust PyTorch command based on your CUDA version or use CPU-only
# See: https://pytorch.org/get-started/locally/
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8
pip install gymnasium numpy pyyaml tqdm tensorboard matplotlib # Add PyYAML, tqdm, tensorboard, matplotlib

# For visualization saving (GIF):
# sudo apt-get update && sudo apt-get install imagemagick # (Linux)
# OR ensure Pillow is sufficient (pip install Pillow)
```

**2. Train Models:**

```bash
# Train DNC on the default maze
python scripts/train.py --config configs/dnc.yaml --device auto

# Train LSTM baseline on the default maze
python scripts/train.py --config configs/lstm.yaml --device auto

# Train on RepeatCopy task (adjust config files first)
# python scripts/train.py --config configs/dnc_copy.yaml --device auto
```
*Training logs and checkpoints will be saved in `runs/` and `saved_models/` respectively, organized by run ID.*

**3. Monitor Training (Optional):**

```bash
tensorboard --logdir runs/
```
*Open your browser to the provided URL (usually `http://localhost:6006`).*

**4. Visualize Memory Access (After Training):**

```bash
# Find a checkpoint file, e.g., saved_models/ProceduralMazeEnv_dnc_.../checkpoint_ep5000.pt
python scripts/visualize.py \
    --config runs/ProceduralMazeEnv_dnc_.../config.yaml `# Path to the config used for the run` \
    --checkpoint saved_models/ProceduralMazeEnv_dnc_.../checkpoint_ep5000.pt \
    --output maze_dnc_memory.gif \
    --device cpu
```

## Potential Applications

*   **Robotics:** Learning complex manipulation sequences, long-horizon planning with persistent state.
*   **Algorithm Learning:** Solving simple programs, graph traversal, or sequence manipulation tasks.
*   **Natural Language Processing:** Story understanding, question answering requiring reasoning over long contexts.
*   **Meta-Learning:** The memory could potentially store task-specific information, allowing faster adaptation to new variations of a task.

## Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs, feature requests, or improvements.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) filefor details.

## References

*   Graves, A., Wayne, G., Reynolds, M. et al. Hybrid computing using a neural network with dynamic external memory. Nature 538, 471â€“476 (2016). [https://doi.org/10.1038/nature20101](https://doi.org/10.1038/nature20101)
